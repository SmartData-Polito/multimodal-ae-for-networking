{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77de2d6b",
   "metadata": {},
   "source": [
    "# <b>Task02 - Darknet Traffic Analysis</b></b>\n",
    "___\n",
    "\n",
    "In the second task, we solve a traffic classification problem related to darknet traffic. Namely, we try to classify group of senders coordinated in targeting darknets. \n",
    "\n",
    "We use the `DARKNET` dataset. It includes 13 categories of known senders, including benign security services, research scanners, and known botnets, as well as an additional class for unknown IP addresses. It consists of one day of data collected from a /24 darknet and includes 14k sender IP addresses, with 5k belonging to known classes and the rest classified as unknown. The measurements for each sender include the sequence of IP addresses as they appear in the trace, statistics of the TCP/UDP ports they target, statistics of packet length. This dataset is characterized by strong class imbalance, with a coefficient of 0.4.\n",
    "\n",
    "**Note** In this notebook we report only the validation of the models and the experiments _without the training_. If you want to inspect our training approach or run again a model training see README (**Training the models** section).\n",
    "\n",
    "# Table of Content\n",
    "- Configuration\n",
    "- Load features\n",
    "- Validate the models\n",
    "- k-nearest-neighborhood class probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe6dbb-5cb5-4d72-9631-ec3811acfb1f",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Before we begin, we need to set up our environment and load the necessary libraries and modules. We also need to specify the paths to the data files and define some global variables that will be used throughout the notebook. \n",
    "\n",
    "The `DEMO` flag controls whether we are running the notebook in demonstration mode (`True`) or full mode (`False`). In demonstration mode some experiments will be run with less samples and the output will not be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822a33e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 10:37:34.628007: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Make mltoolbox and utls reachable from this folder\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import*\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# Features and embeddings paths\n",
    "FEATURES = '../data/task02/features'\n",
    "EMBEDDINGS = '../data/task02/embeddings'\n",
    "INTERIM = '../data/interim'\n",
    "\n",
    "# Demonstrative flag\n",
    "DEMO = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd149fc9",
   "metadata": {},
   "source": [
    "## Load features\n",
    "\n",
    "In this section, we will load the data files that contain the features for our machine learning models. We will use the `pandas` library to read in the CSV files and store the data in dataframes. \n",
    "- The `ipaddress` dataframe will contain the word2vec embeddings for the IP addresses\n",
    "- The `payload` dataframe will contain the payload bytes\n",
    "- The `statistics` dataframe will contain Tstat-style features\n",
    "- The `sequences` dataframe will contain statistical features in sequence referred to each byte. \n",
    "\n",
    "The data in these dataframes will be used as input to our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b25c42f-a76b-45ea-a28d-66670f344b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load ports word2vec embeddings - entity\n",
    "ports=pd.read_csv(f'{FEATURES}/ports.csv', index_col=[0])\n",
    "# Load statistics features - quantity\n",
    "statistics=pd.read_csv(f'{FEATURES}/statistics.csv', index_col=[0])\n",
    "# Load ip address word2vec embeddings - entity\n",
    "ipaddress=pd.read_csv(f'{FEATURES}/ipaddress.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01063a4d-e1d0-4b35-9692-cd41a559410d",
   "metadata": {},
   "source": [
    "Then we merge the dataframes containing our features into a single dataframe called concat. We start by resetting the index of the payload dataframe and dropping the 'label' column. Then, we perform an inner join on the 'index' column with the statistics dataframe, also dropping the 'label' column. We repeat this process for the sequences dataframe and the ipaddress dataframe. Finally, we set the 'index' column as the index of the resulting dataframe. This results in a single dataframe that contains all of the features for our models, with the 'index' column serving as the primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f528cb9-d28e-4bd3-8ca2-2aeeb27c84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the features as raw concatenation\n",
    "concat = ports.reset_index().drop(columns=['label'])\\\n",
    "              .merge(statistics.reset_index().drop(columns=['label']), \n",
    "                     on='index', how='inner')\\\n",
    "              .merge(ipaddress.reset_index(), \n",
    "                     on='index', how='inner')\\\n",
    "              .set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a1317-6b74-4a82-a378-9960a33ef2d5",
   "metadata": {},
   "source": [
    "Finally, we collect the features sets in a dictionary, we load the stratified-k-folds order we provide and retrieve the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89fb0f4-04ee-4f47-9553-09ee2d0a5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Collect the features in a dictionary\n",
    "features = {'ports':ports, 'statistics':statistics,\n",
    "            'ipaddress':ipaddress, 'rawcat':concat, 'mae':None}\n",
    "\n",
    "# Load stratified k folds\n",
    "kfolds = joblib.load(f'../data/task02/skfolds/folds.save')\n",
    "\n",
    "# Get the number of classes\n",
    "n_classes = ipaddress.value_counts('label').shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e42bb",
   "metadata": {},
   "source": [
    "## Validate the models\n",
    "\n",
    "After having trained the models through the training scripts, we need to validate them.\n",
    "\n",
    "The following function is responsible for evaluating the pre-trained classifiers using cross-validation.  The model predicts the labels of each one of the provided fold at a time. It then generates a summary of the model's performance on the validation set in the form of a classification report, which includes metrics such as precision, recall, and f1-score. The function can be called multiple times with different values of K in order to validate the model's performance on all of the folds of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f205b4-2f4c-4bda-83b0-a7e817227fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from mltoolbox.classification import DeepClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def validate_single_run(feature, fname, K, pbar):\n",
    "    # Retrieve the training and validation samples from the k-folds order\n",
    "    X_train, X_val, y_train, y_val = get_datasets(kfolds, K, feature)\n",
    "    \n",
    "    # Load the classifier model from the specified file path\n",
    "    mpath = f'../data/task02/classifiers/{fname}_k{K}'\n",
    "    classifier = DeepClassifier(_load_model=True, model_path=mpath)\n",
    "    \n",
    "    # Use the classifier to predict labels for the validation set\n",
    "    y_pred = classifier.predict(X_val, scale_data=True)\n",
    "    report = classification_report(y_val, y_pred, labels=np.unique(y_val), \n",
    "                                   output_dict=True)\n",
    "    \n",
    "     # Extract the macro average f1-score from the report\n",
    "    f1 = round(report['macro avg']['f1-score'], 2)\n",
    "    \n",
    "    # Update the progress bar object and set the postfix message\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({'current fold':K, 'macro avg. f1': f1})\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb86bff-2c6f-403b-a17a-8479d9c88f16",
   "metadata": {},
   "source": [
    "Now we can validate the models. Namely, we run a full stratified-k-folds cross validation over:\n",
    "- Raw features independently\n",
    "- Concatenation of the raw features\n",
    "- Multi-modal embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3670704d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1641e3ff05d486fbb780e49d4d8034e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8825b53abf03456faa013fd1f890dbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c3e39bdb4a4ac1b4a7252ae327579a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce7a4949bdc49a6b2dbf9e67eefdfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da414524207426594bb777e154afdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fname, feature in features.items():\n",
    "    # Initialize a progress bar with a total of 5 iterations (skf)\n",
    "    pbar = tqdm(total=5)\n",
    "    pbar.set_description(f'Validating {fname}')\n",
    "    \n",
    "    # Iterate over the stratified folds\n",
    "    for K in range(5):\n",
    "        if fname == 'mae':\n",
    "            # Load the pre-trained multimodal embeddings\n",
    "            feature=pd.read_csv(f'{EMBEDDINGS}/mae_embeddings_k{K}.csv', \n",
    "                               index_col=[0])\n",
    "            \n",
    "        # Validate the classifier getting the classification metrics\n",
    "        report = validate_single_run(feature, fname, K, pbar)\n",
    "        \n",
    "        # Save the report to a CSV file if not demonstrative\n",
    "        if not DEMO:\n",
    "            pd.DataFrame(report).T.to_csv(f'{INTERIM}/task02_{fname}_deep_k{K}.csv')\n",
    "            \n",
    "    # Close the progress bar       \n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1771e2",
   "metadata": {},
   "source": [
    "## k-nearest-neighborhood class probability\n",
    "\n",
    "We now evaluate the embeddings space through the k-nearest-neighborhood class probability. It consists on applying a k-nearest-neighbors classifier on the whole dataset. Then, for each sample whose label is different from `unknown` (if present) we compute the probability of having samples with the same label in their neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5e79d1e-782a-4356-a07f-c00a3f15f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltoolbox.classification import KnnClassifier\n",
    "from mltoolbox.metrics import k_class_proba_report\n",
    "\n",
    "def kpc_single_run(feature, fname, K, k, pbar):\n",
    "    # Retrieve the training and validation datasets for the current fold\n",
    "    X_train, X_val, y_train, y_val = get_datasets(kfolds, K, feature)\n",
    "    X, y = np.vstack([X_train, X_val]), np.hstack([y_train, y_val])\n",
    "    \n",
    "    # If demonstrative load less samples\n",
    "    if DEMO: X, y = X[:3000], y[:3000]\n",
    "    \n",
    "    # Train a KNN classifier with cosine similarity and the specified number \n",
    "    # of neighbors\n",
    "    knn = KnnClassifier(n_neighbors=k, metric='cosine')\n",
    "    knn.fit(X, y, scale_data=True)\n",
    "    \n",
    "    # Keep only the samples with labels other than 'unknown' and predict labels\n",
    "    to_keep = np.where(y!='unknown')[0].reshape(-1, 1)\n",
    "    pcs = knn.predict_proba(to_keep)\n",
    "    \n",
    "    # Generate a report with the k-class probabilities\n",
    "    y_true = y[np.ravel(to_keep)]# Extract the true labels\n",
    "    report = k_class_proba_report(y_true, pcs, output_dict=True)\n",
    "    \n",
    "    # Extract the macro average k-class probability from the report\n",
    "    kpc = report['macro avg']['kpc']\n",
    "    \n",
    "    # Update the progress bar and set the postfix message\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({'current fold':K, f'{k}Pc': kpc})\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89edc1da-0123-409a-a73b-1f78ebf1ae7a",
   "metadata": {},
   "source": [
    "We evaluate the neighborhood for the Multi-modal embeddings and the concatenation of the raw features. We average the experiment on the 5 folds. \n",
    "\n",
    "Note that, since we want to evaluate the embeddings neighborhood, we do not need to distinguish between training/validation samples, thus we merge together the subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa16574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3545fa963be47eeb31f63c274207df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe01bc770afa4b2081947158cb68d390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ranges = range(1, 20) # Ranges of neighborhood radius\n",
    "# If demonstrative limit the runs\n",
    "if DEMO: ranges = range(1, 5)\n",
    "\n",
    "# Evaluate only the 'rawcat' and 'mae' features\n",
    "for fname, feature in features.items():\n",
    "    if fname in ['rawcat', 'mae']:\n",
    "        # Initialize a progress bar\n",
    "        pbar = tqdm(total=len(ranges)*5)\n",
    "        pbar.set_description(f'Evaluating {fname} neighborhood')\n",
    "        \n",
    "        # Iterate over the stratified folds\n",
    "        for K in range(5):\n",
    "            for k in ranges: # Try different neighborhood radious\n",
    "                if fname == 'mae':\n",
    "                    # Load the pre-trained multimodal embeddings\n",
    "                    feature=pd.read_csv(f'{EMBEDDINGS}/mae_embeddings_k{K}.csv', \n",
    "                                       index_col=[0])\n",
    "                # Compute the class probability\n",
    "                report = kpc_single_run(feature, fname, K, k, pbar)\n",
    "                \n",
    "                # Save the report to a CSV file if not demonstrative\n",
    "                if not DEMO:\n",
    "                    pd.DataFrame(report).T.to_csv(f'{INTERIM}/task02_{fname}_{k}pc_k{K}.csv')\n",
    "        \n",
    "        # Close the progress bar \n",
    "        pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "81dc8a169688350846d75f5b64795c14310fd5665d0ae2bea435e43803ad8b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
