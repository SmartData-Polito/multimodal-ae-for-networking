{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c71dcb-6af7-4f96-b2fb-42279015f282",
   "metadata": {},
   "source": [
    "# <b>Grid Search</b>\n",
    "___\n",
    "\n",
    "In this notebook we report the code used to evaluate the grid search. Note that we run the grid search only on the first task (Traffic Application Classification -- `MIRAGE` dataset.\n",
    "\n",
    "**Note** In this notebook we report only the validation of the models and the experiments _without the training_. If you want to inspect our training approach or run again a model training see README (**Training the models** section).\n",
    "\n",
    "# Table of Content\n",
    "- Configuration\n",
    "- Load features\n",
    "- Validate the models\n",
    "- Retrieve number of trainables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c7a01-b53a-4da2-a76e-90f0a842eb6b",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Before we begin, we need to set up our environment and load the necessary libraries and modules. We also need to specify the paths to the data files and define some global variables that will be used throughout the notebook. \n",
    "\n",
    "The `DEMO` flag controls whether we are running the notebook in demonstration mode (`True`) or full mode (`False`). In demonstration mode some experiments will be run with less samples and the output will not be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce61af60-9c0b-420a-b1ae-c367bc80a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 11:02:09.272330: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Make mltoolbox and utls reachable from this folder\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import*\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# Features and embeddings paths\n",
    "FEATURES = '../data/task01/features'\n",
    "EMBEDDINGS = '../data/task01/embeddings'\n",
    "INTERIM = '../data/interim'\n",
    "MAE = '../data/task01/mae'\n",
    "\n",
    "# Demonstrative flag\n",
    "DEMO = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945563a-20fc-4d19-8586-705f90b9e0d1",
   "metadata": {},
   "source": [
    "## Load features\n",
    "\n",
    "In this section, we will load the data files that contain the features for our machine learning models. We will use the `pandas` library to read in the CSV files and store the data in dataframes. \n",
    "- The `ipaddress` dataframe will contain the word2vec embeddings for the IP addresses\n",
    "- The `payload` dataframe will contain the payload bytes\n",
    "- The `statistics` dataframe will contain Tstat-style features\n",
    "- The `sequences` dataframe will contain statistical features in sequence referred to each byte. \n",
    "\n",
    "The data in these dataframes will be used as input to our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12e98a3-ff19-49de-b54e-98f69a90c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load ip address word2vec embeddings - entity\n",
    "ipaddress=pd.read_csv(f'{FEATURES}/ipaddress.csv', index_col=[0])\n",
    "# Load payload bytes - quantity\n",
    "payload=pd.read_csv(f'{FEATURES}/payload.csv', index_col=[0])\n",
    "# Load statistics features - quantity\n",
    "statistics=pd.read_csv(f'{FEATURES}/statistics.csv', index_col=[0])\n",
    "# Load statistics sequences - quantity\n",
    "sequences=pd.read_csv(f'{FEATURES}/sequences.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ad592-1606-486b-8723-520c34e8408a",
   "metadata": {},
   "source": [
    "Then we merge the dataframes containing our features into a single dataframe called concat. We start by resetting the index of the payload dataframe and dropping the 'label' column. Then, we perform an inner join on the 'index' column with the statistics dataframe, also dropping the 'label' column. We repeat this process for the sequences dataframe and the ipaddress dataframe. Finally, we set the 'index' column as the index of the resulting dataframe. This results in a single dataframe that contains all of the features for our models, with the 'index' column serving as the primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd152a4-26a5-4c1b-8c9c-54093971a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the features as raw concatenation\n",
    "concat = payload.reset_index().drop(columns=['label'])\\\n",
    "                .merge(statistics.reset_index().drop(columns=['label']), \n",
    "                       on='index', how='inner')\\\n",
    "                .merge(sequences.reset_index().drop(columns=['label']), \n",
    "                       on='index', how='inner')\\\n",
    "                .merge(ipaddress.reset_index(), on='index', how='inner')\\\n",
    "                .set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f510344-f5a2-44e0-9621-ead0bacb613d",
   "metadata": {},
   "source": [
    "Finally, we collect the features sets in a dictionary, we load the stratified-k-folds order we provide and retrieve the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1672f94d-7911-461b-8684-20025616c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Collect the features in a dictionary\n",
    "features = {'payload':payload, 'statistics':statistics,\n",
    "            'sequences':sequences, 'ipaddress':ipaddress,\n",
    "            'rawcat':concat, 'mae':None}\n",
    "\n",
    "# Load stratified k folds\n",
    "kfolds = joblib.load(f'../data/task01/skfolds/folds.save')\n",
    "\n",
    "# Get the number of classes\n",
    "n_classes = ipaddress.value_counts('label').shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57543b-3e5e-4a64-be47-98c3472fa722",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Validate the models\n",
    "\n",
    "After having trained the models through the training scripts, we need to validate them.\n",
    "\n",
    "The following function is responsible for evaluating the pre-trained classifiers using cross-validation.  The model predicts the labels of each one of the provided fold at a time. It then generates a summary of the model's performance on the validation set in the form of a classification report, which includes metrics such as precision, recall, and f1-score. The function can be called multiple times with different values of K in order to validate the model's performance on all of the folds of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e02502-55ca-4aeb-a44a-652a68334fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from mltoolbox.classification import DeepClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def validate_single_run(feature, mpath, K, pbar):\n",
    "    # Retrieve the training and validation samples from the k-folds order\n",
    "    X_train, X_val, y_train, y_val = get_datasets(kfolds, K, feature)\n",
    "    \n",
    "    # Load the classifier model from the specified file path\n",
    "    classifier = DeepClassifier(_load_model=True, model_path=mpath)\n",
    "    \n",
    "    # Use the classifier to predict labels for the validation set\n",
    "    y_pred = classifier.predict(X_val, scale_data=True)\n",
    "    report = classification_report(y_val, y_pred, labels=np.unique(y_val), \n",
    "                                   output_dict=True)\n",
    "    \n",
    "     # Extract the macro average f1-score from the report\n",
    "    f1 = round(report['macro avg']['f1-score'], 2)\n",
    "    \n",
    "    mname = (mpath.split('/')[-1]).replace('gridsearch_', '')\n",
    "    # Update the progress bar object and set the postfix message\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({'current model':mname, \n",
    "                      'macro avg. f1': f1})\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b59c8-7631-40e9-9620-ce3799045cad",
   "metadata": {},
   "source": [
    "Run the grid search validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "790239dc-aa5e-44d4-ba42-442ddec4c0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b8fb4c8f04476aa9cbf2950ed27724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de76b60200cc488184e00950468fe5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33a10db57fa48c5b60118f18b52eb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477f94c8797843d99f4f94765843a433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4744e27aa664b96ba8936fd66c3ea86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate over the stratified folds\n",
    "for K in range(5):\n",
    "    # Initialize a progress bar with a total of 16 iterations (gs)\n",
    "    pbar = tqdm(total=16)\n",
    "    pbar.set_description(f'Validating Fold {K}')\n",
    "\n",
    "    for l1 in [32, 64, 128, 256]:\n",
    "        for l4 in [32, 64, 128, 256]:\n",
    "            # Load the pre-trained multimodal embeddings\n",
    "            feature=pd.read_csv(f'{EMBEDDINGS}/gridsearch_{l1}_{l4}_k{K}.csv', \n",
    "                               index_col=[0])\n",
    "            mpath = f'../data/task01/classifiers/gridsearch_{l1}_{l4}_k{K}'\n",
    "            # Validate the classifier getting the classification metrics\n",
    "            report = validate_single_run(feature, mpath, K, pbar)\n",
    "\n",
    "            # Save the report to a CSV file if not demonstrative\n",
    "            if not DEMO:\n",
    "                pd.DataFrame(report).T.to_csv(f'{INTERIM}/gridsearch_{l1}_{l4}_k{K}.csv')\n",
    "\n",
    "    # Close the progress bar       \n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd834d-115a-46d2-9875-65485f9ef24e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retrieve number of trainables\n",
    "\n",
    "Finally, we evaluate the multi-modal encoder size. Namely, we iterate through different combinations of values for hyperparameters `l1` and `l4` and for each combination, we loads a pre-trained MAE, extract the encoder portion of the model, and count the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e058ef-f3c2-4b4d-84b8-48c3bf0755c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d9b593d4d742e18827e1a8420a621e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import models\n",
    "from mltoolbox.representation import MultimodalAE\n",
    "\n",
    "report = []\n",
    "# Initialize a progress bar with a total of 5 iterations (skf)\n",
    "pbar = tqdm(total=16)\n",
    "pbar.set_description(f'Counting trainables')\n",
    "\n",
    "for l1 in [32, 64, 128, 256]:\n",
    "    for l4 in [32, 64, 128, 256]:\n",
    "        # Retrieve the multimodal AE\n",
    "        mae = MultimodalAE(model_path=f'{MAE}/gridsearch_{l1}_{l4}_k0',\n",
    "                           _load_model=True)\n",
    "        \n",
    "        # Extract only the encoder\n",
    "        i,o = mae.extract_encoder()\n",
    "        \n",
    "        # Get the number of trainables\n",
    "        params = models.Model(i,o).count_params()/1e4\n",
    "        report.append((l1, l4, params))\n",
    "        \n",
    "        # Update the progress bar object and set the postfix message\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({'l1':l1, 'l4':l4, 'params [x1e4]':params})\n",
    "# Close the progressbar\n",
    "pbar.close()\n",
    "\n",
    "# If not demonstrative, save report to file\n",
    "if not DEMO:\n",
    "    df = pd.DataFrame(report, columns=['l1', 'l4', 'trainables'])\n",
    "    pd.DataFrame(df).to_csv(f'{INTERIM}/trainables.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7752e352c355b3ac8de75594c3972c357c3b0b08f713d3e5a9c1d8631c75ec52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
